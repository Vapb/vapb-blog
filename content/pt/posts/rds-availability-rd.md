---
title: "RDS PostgreSQL: High Availability and Disaster Recovery"
author: "vapb"
description: "Guide to RDS availability: Multi-AZ configurations, failover mechanisms, snapshots, read replicas, and disaster recovery strategies for mission-critical databases."
date: 2025-11-02
tags: ["AWS", "RDS", "PostgreSQL"]
toc: true
---

## 1. Por Que Alta Disponibilidade Importa
Imagine acordar Ã s 3h da manhÃ£ com alertas de que seu banco de dados de produÃ§Ã£o estÃ¡ fora do ar.
Seu e-commerce estÃ¡ offline. Cada minuto custa milhares de reais em receita perdida.
Clientes estÃ£o frustrados. Seu time estÃ¡ em pÃ¢nico.

Ã‰ por isso que Alta Disponibilidade (HA) nÃ£o Ã© opcional para sistemas de produÃ§Ã£o.

## 2. OpÃ§Ãµes de Alta Disponibilidade no RDS
O RDS oferece trÃªs modelos de deployment, cada um com diferentes garantias de disponibilidade.

### 2.1. Single-AZ (Sem Standby) âŒ

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Availability   â”‚
    â”‚    Zone A       â”‚
    â”‚                 â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Primary   â”‚  â”‚
    â”‚  â”‚ Instance  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Sem redundÃ¢ncia!
```

O que acontece se a primÃ¡ria falhar:
* RDS detecta a falha
* RDS provisiona nova instÃ¢ncia EC2
* RDS anexa volumes EBS
* PostgreSQL inicializa
* Banco de dados fica disponÃ­vel

â° Downtime: Tipicamente 10-30 minutos (Ã s vezes mais)

Quando usar:
* Ambientes dev/test âœ…
* Workloads nÃ£o-crÃ­ticos sensÃ­veis a custo âœ…
* Bancos que podem tolerar downtime prolongado âœ…

âš ï¸ Riscos do Single-AZ
* Falha de hardware â†’ 10-30 min de downtime
* Falha de AZ â†’ Potencialmente horas de downtime
* Janelas de manutenÃ§Ã£o â†’ Downtime durante upgrades
* Sem failover automÃ¡tico

**ConclusÃ£o: Single-AZ NÃƒO Ã© pronto para produÃ§Ã£o.**


### 2.2. Multi-AZ com Um Standby (SÃ­ncrono) âœ…
```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Availability  â”‚        â”‚   Availability  â”‚
    â”‚     Zone A      â”‚        â”‚      Zone B     â”‚
    â”‚                 â”‚        â”‚                 â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚ Primary   â”‚  â”‚â—„â”€â”€â”€â”€â”€â”€â–ºâ”‚  â”‚ Standby   â”‚  â”‚
    â”‚  â”‚ Instance  â”‚  â”‚ Replic â”‚  â”‚ Instance  â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚ SÃ­nc   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚        â”‚        â”‚        â”‚                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Apps conectam aqui
             â–¼
       DNS Endpoint
       mydb.abc.rds.amazonaws.com
```

Este Ã© o setup padrÃ£o de produÃ§Ã£o para a maioria dos workloads.

Fluxo de ReplicaÃ§Ã£o SÃ­ncrona:
1. App envia: INSERT INTO users...
2. PrimÃ¡ria recebe a escrita
3. PrimÃ¡ria envia para standby: "Vou commitar isso"
4. Standby reconhece: "Recebi, persistido"
5. PrimÃ¡ria faz commit
6. PrimÃ¡ria responde para app: "Sucesso!"

Escrita sÃ³ confirmada apÃ³s standby reconhecer
BenefÃ­cios:
* Zero perda de dados (RPO = 0)
* Failover rÃ¡pido (RTO = 1-2 min)
* AutomÃ¡tico (sem intervenÃ§Ã£o manual)
* Mesmo endpoint (mudanÃ§a de DNS, sem alteraÃ§Ãµes no app)

#### 2.2.1. CenÃ¡rio de Failover 1: InstÃ¢ncia primÃ¡ria falha

O que acontece:
* PrimÃ¡ria trava
* Health check do RDS detecta falha
* RDS inicia failover
* DNS aponta para standby
* Standby promovida a primÃ¡ria
* Novas conexÃµes aceitas âœ…

âš ï¸ Downtime total: ~1-2 minutos

Sua aplicaÃ§Ã£o vÃª:
* ConexÃµes existentes: Dropadas (precisam reconectar)
* Novas conexÃµes: Breve rejeiÃ§Ã£o, depois sucesso
* Perda de dados: ZERO (tudo estava sincronizado)

RDS automaticamente:
* Promove standby a primÃ¡ria
* Atualiza DNS (sem mudanÃ§a de IP para o app)
* ComeÃ§a a reconstruir novo standby em background

#### 2.2.2. CenÃ¡rio 2: InstÃ¢ncia standby falha

O que acontece:
* Impacto: NENHUM na sua aplicaÃ§Ã£o

AÃ§Ãµes do RDS:
1. Detecta falha do standby
2. PrimÃ¡ria continua servindo trÃ¡fego normalmente
3. RDS provisiona novo standby em background
4. SincronizaÃ§Ã£o resume automaticamente

#### 2.2.3. CenÃ¡rio 3: AZ inteira falha

O que acontece:
* Se AZ da PrimÃ¡ria falha:
  * Standby em AZ diferente assume
  * Tempo de failover: ~1-2 minutos
  * Zero perda de dados
* Se AZ do Standby falha:
  * PrimÃ¡ria nÃ£o afetada
  * RDS reconstrÃ³i standby em AZ saudÃ¡vel
  * Zero impacto na aplicaÃ§Ã£o

{{< details title="O que causa failover automÃ¡tico?" >}}
RDS faz failover automaticamente para:
* Falhas de infraestrutura:
  * Falha de hardware da instÃ¢ncia primÃ¡ria
  * Falha de storage subjacente
  * Queda em nÃ­vel de AZ
  * Perda de conectividade de rede entre AZs
* OperaÃ§Ãµes de manutenÃ§Ã£o:
  * Patching de SO (aplicado no standby primeiro, depois failover)
  * Upgrades de engine do banco (minimiza downtime)

NÃƒO causa failover âŒ :
* Queries de longa duraÃ§Ã£o (problema do PostgreSQL, nÃ£o infraestrutura)
* Deadlocks (problema de lÃ³gica da aplicaÃ§Ã£o/banco)
* Out of connections (problema de configuraÃ§Ã£o)
* Disco cheio (precisa aumentar storage)

Para problemas em nÃ­vel de banco: RDS reinicia, nÃ£o faz failover.
{{< /details >}}

{{< hint info >}}
ğŸ¯ Vantagens Operacionais
* Backups rodam no standby
* ManutenÃ§Ã£o aplicada no standby primeiro
   * Standby recebe patch/upgrade
   * Failover acontece (1-2 min de downtime)
   * PrimÃ¡ria antiga vira novo standby
   * Novo standby recebe patch
   * Resultado: Downtime mÃ­nimo para manutenÃ§Ã£o
* Garantia de SLA
  * 99,95% de uptime mensal no SLA
  * Traduz para ~22 minutos mÃ¡x de downtime por mÃªs
* Mesmo endpoint
  * DNS: mydb.abc.rds.amazonaws.com
  * Sem mudanÃ§as na aplicaÃ§Ã£o apÃ³s failover
  * Connection string permanece a mesma
{{< /hint >}}


### 2.3. Multi-AZ DB Cluster (Semi-SÃ­ncrono) ğŸš€
```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     AZ A     â”‚   â”‚     AZ B     â”‚   â”‚     AZ C     â”‚
    â”‚              â”‚   â”‚              â”‚   â”‚              â”‚
    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚ â”‚ Primary  â”‚ â”‚   â”‚ â”‚ Readable â”‚ â”‚   â”‚ â”‚ Readable â”‚ â”‚
    â”‚ â”‚ Instance â”‚â—„â”œâ”€â”€â”€â”¤â–ºâ”‚ Standby  â”‚ â”‚   â”‚ â”‚ Standby  â”‚ â”‚
    â”‚ â”‚          â”‚ â”‚   â”‚ â”‚    #1    â”‚ â”‚   â”‚ â”‚    #2    â”‚ â”‚
    â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚      â”‚NVMe   â”‚   â”‚      â”‚NVMe   â”‚   â”‚      â”‚NVMe   â”‚
    â”‚      â”‚SSD    â”‚   â”‚      â”‚SSD    â”‚   â”‚      â”‚SSD    â”‚
    â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                  â”‚                  â”‚
           â–¼                  â–¼                  â–¼
      Write Endpoint     Reader Endpoint    Reader Endpoint
```

Esta Ã© a opÃ§Ã£o premium para RTO ultra-baixo e escalabilidade de leitura integrada.

ReplicaÃ§Ã£o Semi-SÃ­ncrona:
1. App envia: INSERT INTO orders...
2. PrimÃ¡ria recebe a escrita
3. PrimÃ¡ria envia para AMBOS standbys
4. PrimÃ¡ria aguarda QUALQUER UM dos standbys reconhecer
5. PrimÃ¡ria faz commit (nÃ£o espera pelos dois)
6. PrimÃ¡ria responde ao app: "Sucesso!"
7. Apenas um standby precisa confirmar (mais rÃ¡pido que sync total)

DiferenÃ§as-chave do Multi-AZ com Um Standby:
* 2 standbys em vez de 1 (trÃªs AZs no total)
* Standbys sÃ£o legÃ­veis (podem servir queries SELECT)
* Failover mais rÃ¡pido (<35 segundos vs 1-2 minutos)
* SSDs NVMe locais (melhor performance de I/O)
* Reader endpoint (balanceamento automÃ¡tico)

{{< hint info >}}
ğŸ¯ Vantagens Operacionais
1. Failover Ultra-RÃ¡pido (<35 segundos)
2. Escalabilidade de Leitura Integrada
3. Reader Endpoint AutomÃ¡tico
4. Melhor Performance
{{< /hint >}}

Quando Usar Multi-AZ Cluster: 
* RTO < 35 segundos necessÃ¡rio
* AplicaÃ§Ãµes mission-critical
* ServiÃ§os financeiros, saÃºde
* E-commerce durante picos sazonais
* Workload pesado em leitura

| Funcionalidade | Single-AZ | Multi-AZ (1 Standby) | Multi-AZ Cluster |
|----------------|-----------|----------------------|------------------|
| **Availability Zones** | 1 | 2 | 3 |
| **ReplicaÃ§Ã£o** | Nenhuma | SÃ­ncrona | Semi-sÃ­ncrona |
| **RTO** | 10-30 min | 1-2 min | <35 seg |
| **RPO** | Minutos a horas | 0 (sem perda de dados) | 0 (sem perda de dados) |
| **Standbys LegÃ­veis** | N/A | âŒ NÃ£o | âœ… Sim (2) |
| **Failover AutomÃ¡tico** | âŒ NÃ£o | âœ… Sim | âœ… Sim |
| **Tipo de Storage** | EBS | EBS | NVMe SSD Local |
| **SLA** | Nenhum | 99,95% | 99,99% |
| **Custo** | $ | $$ (2x Single-AZ) | $$$$ (4x Single-AZ) |
| **Melhor Para** | Dev/test | ProduÃ§Ã£o (padrÃ£o) | Mission-critical |

## 3. EstratÃ©gias de RecuperaÃ§Ã£o de Desastres

Alta Disponibilidade protege contra falhas de infraestrutura. RecuperaÃ§Ã£o de Desastres protege contra eventos catastrÃ³ficos: quedas em toda regiÃ£o, deleÃ§Ãµes acidentais, corrupÃ§Ã£o de dados, ransomware.

### 3.1. Snapshots do RDS
Snapshots sÃ£o backups point-in-time do seu banco de dados inteiro.

{{< details title="Snapshots automatizados" >}}
```
Dia 1:
â”œâ”€ 00:00 - Snapshot completo tirado
â””â”€ Durante o dia: Logs transacionais capturados

Dia 2:
â”œâ”€ 00:00 - Snapshot incremental (apenas mudanÃ§as)
â””â”€ Durante o dia: Logs transacionais capturados

Dia 3:
â”œâ”€ 00:00 - Snapshot incremental
â””â”€ E assim por diante...
```

Funcionalidades:
* Backups automÃ¡ticos diÃ¡rios
* Incrementais (apenas blocos alterados)
* Inclui logs transacionais (para restore point-in-time)
* RetenÃ§Ã£o padrÃ£o: 7 dias (configurÃ¡vel atÃ© 35 dias)
* Janela de backup: Especifique horÃ¡rio preferido (perÃ­odo de baixo trÃ¡fego)

Point-in-Time Restore (PITR) -> CenÃ¡rio: DELETE acidental Ã s 14:47.
VocÃª pode restaurar para:
* 14:46 (antes do DELETE)
* 14:30 (30 min antes)
* 10:00 (hoje de manhÃ£)
* Qualquer incremento de 5 minutos dentro do perÃ­odo de retenÃ§Ã£o

Custo:
* IncluÃ­do no preÃ§o do RDS
* Custo de storage: Mesmo que storage provisionado
* Exemplo: DB de 100 GB = ~R$ 50/mÃªs para backups
{{< /details >}}

{{< details title="Snapshots manuais" >}}
```
Snapshots Automatizados:
â”œâ”€ Acontecem automaticamente diariamente
â”œâ”€ Deletados apÃ³s perÃ­odo de retenÃ§Ã£o
â”œâ”€ Suportam restore point-in-time
â””â”€ Vinculados Ã  instÃ¢ncia

Snapshots Manuais:
â”œâ”€ VocÃª os dispara
â”œâ”€ NUNCA deletados automaticamente
â”œâ”€ SEM restore point-in-time (apenas o momento do snapshot)
â””â”€ Independentes da instÃ¢ncia (persistem apÃ³s deleÃ§Ã£o)
```

Quando usar snapshots manuais:
*  Antes de mudanÃ§as maiores
*  Requisitos de compliance/auditoria
*  Antes de deletar instÃ¢ncia

Custo:
* Paga apenas pelo storage
* $0,095/GB/mÃªs em us-east-1
* Snapshot de 100 GB = $9,50/mÃªs
{{< /details >}}

{{< hint info >}}
ğŸ¯ EstratÃ©gia de Snapshots para ProduÃ§Ã£o
* Configure backups automatizados
* Tire snapshots manuais antes de mudanÃ§as:
  * MigraÃ§Ãµes de schema
  * Upgrades de versÃ£o maior
  * MudanÃ§as de configuraÃ§Ã£o
  * Antes de deletar instÃ¢ncia
* Copie snapshots cross-region
* Teste restores regularmente
{{< /hint >}}

#### 3.1.1. Processo de Restore de Snapshot

Timeline:
1. Iniciar restore (chamada API): 1 minuto
2. RDS provisiona nova instÃ¢ncia: 5-10 minutos
3. Restore dados do snapshot: 10-60 minutos (depende do tamanho)
4. InstÃ¢ncia fica disponÃ­vel: Total 15-70 minutos

Fatores que afetam o tempo:
* Tamanho do banco (maior = mais longo)
* Classe da instÃ¢ncia (maior = restore mais rÃ¡pido)
* Carga da regiÃ£o (horÃ¡rios de pico podem ser mais lentos)

ApÃ³s restore -> Nova instÃ¢ncia:
* Endpoint diferente (precisa atualizar connection strings)
* Mesmos dados do momento do snapshot
* Mesma configuraÃ§Ã£o (classe de instÃ¢ncia, parÃ¢metros)
* InstÃ¢ncia original ainda rodando (vocÃª escolhe qual manter)

### 3.2. Read Replicas para DR
Read replicas servem a DOIS propÃ³sitos:
* Escalabilidade de leitura (descarregar queries SELECT)
* RecuperaÃ§Ã£o de desastres (pode ser promovida a standalone)

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  us-east-1      â”‚        â”‚  us-west-2      â”‚
  â”‚                 â”‚        â”‚                 â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚  â”‚ Primary   â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â”‚  Read     â”‚  â”‚
  â”‚  â”‚ (Source)  â”‚  â”‚ Async  â”‚  â”‚  Replica  â”‚  â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚ Replic â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚        â”‚        â”‚        â”‚                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    TrÃ¡fego de escrita          
```

ReplicaÃ§Ã£o AssÃ­ncrona:
1. App escreve na primÃ¡ria
2. PrimÃ¡ria faz commit imediatamente
3. PrimÃ¡ria envia mudanÃ§a para rÃ©plica
4. RÃ©plica aplica mudanÃ§a (eventualmente)
5. RÃ©plica pode estar atrasada em relaÃ§Ã£o Ã  primÃ¡ria (segundos a minutos) âš ï¸

CaracterÃ­sticas-chave:
* ReplicaÃ§Ã£o async (sem impacto na latÃªncia de escrita)
* Pode estar na mesma regiÃ£o ou cross-region
* Pode ter classe de instÃ¢ncia diferente da primÃ¡ria
* Pode ser promovida a instÃ¢ncia standalone
* Lag de replicaÃ§Ã£o possÃ­vel (monitore de perto)

####  3.2.1. Read Replicas In-Region vs Cross-Region
| CaracterÃ­stica | In-Region | Cross-Region |
|----------------|-----------|--------------|
| **Topologia** | PrimÃ¡ria (us-east-1a) â†’ RÃ©plica (us-east-1b) | PrimÃ¡ria (us-east-1) â†’ RÃ©plica (eu-west-1) |
| **Lag de ReplicaÃ§Ã£o** | <1 segundo (tipicamente) | 1-5 segundos (depende da distÃ¢ncia) |
| **Custo** | ~2x custo da instÃ¢ncia | 2x instÃ¢ncia + transferÃªncia de dados ($0,02/GB) |
| **TransferÃªncia de Dados** | âœ… Sem cobranÃ§a | âŒ Cobrado ($0,02/GB out) |
| **LatÃªncia** | Muito baixa (~ms) | VariÃ¡vel (10-200ms conforme distÃ¢ncia) |
| **Caso de Uso Principal** | Escalabilidade de leitura | RecuperaÃ§Ã£o de desastres + leituras globais |
| **Melhor Para** | Descarregar reads da primÃ¡ria | DR cross-region, compliance, usuÃ¡rios globais |


Quando usar cross-region:
* RecuperaÃ§Ã£o de desastres (proteÃ§Ã£o contra falha em toda regiÃ£o)
* Compliance (requisitos de residÃªncia de dados)
* AplicaÃ§Ã£o global (servir usuÃ¡rios da regiÃ£o mais prÃ³xima)
* Menor latÃªncia de leitura para usuÃ¡rios geograficamente distribuÃ­dos

#### 3.2.2. Dimensionamento de Read Replica
RÃ©plica pode ser de tamanho diferente da primÃ¡ria

BenefÃ­cios replica maior:
* Lida com workload pesado de leitura facilmente
* Sem degradaÃ§Ã£o de performance
* Bom para queries analÃ­ticas

Risco replica menor:
* RÃ©plica nÃ£o consegue acompanhar primÃ¡ria
* Lag de replicaÃ§Ã£o aumenta
* Eventualmente rÃ©plica fica muito atrasada
* Ruim para recuperaÃ§Ã£o de desastres!

Regra prÃ¡tica: RÃ©plica deve ser â‰¥ mesma classe que primÃ¡ria para propÃ³sitos de DR.

## 4. Alta Disponibilidade vs RecuperaÃ§Ã£o de Desastres

| Aspecto | Alta Disponibilidade (HA) | RecuperaÃ§Ã£o de Desastres (DR) |
|---------|---------------------------|-------------------------------|
| **PropÃ³sito** | Proteger contra falhas de infraestrutura | Proteger contra eventos catastrÃ³ficos |
| **Tecnologia** | Multi-AZ (replicaÃ§Ã£o sync) | Snapshots + Read Replicas (async) |
| **RPO** | 0 (sem perda de dados) | Minutos a horas (depende do backup) |
| **RTO** | 1-2 min (35 seg para cluster) | Horas (restore de snapshot) |
| **Escopo** | Mesma regiÃ£o, AZs diferentes | Cross-region |
| **ReplicaÃ§Ã£o** | SÃ­ncrona | AssÃ­ncrona |
| **Classe de InstÃ¢ncia** | Mesma que primÃ¡ria | Pode diferir |
| **Custo** | 2x custo da instÃ¢ncia | 2x + storage + transferÃªncia de dados |
| **Failover** | AutomÃ¡tico | Manual (promover rÃ©plica) |
| **Protege Contra** | Falha de AZ, problemas de hardware | Falha de regiÃ£o, corrupÃ§Ã£o de dados, acidentes |


## 5. ConclusÃ£o
Alta Disponibilidade e RecuperaÃ§Ã£o de Desastres nÃ£o sÃ£o opcionais para bancos de dados de produÃ§Ã£oâ€”sÃ£o seguro essencial contra falhas inevitÃ¡veis.

8.1. Principais ConclusÃµes
Alta Disponibilidade:
* Use Multi-AZ para toda produÃ§Ã£o (RTO: 1-2 min, RPO: 0) âœ… 
* Use Multi-AZ Cluster para mission-critical (RTO: <35 seg) ğŸš€ 
* Nunca use Single-AZ para produÃ§Ã£o âŒ 

RecuperaÃ§Ã£o de Desastres:
* Habilite backups automatizados (retenÃ§Ã£o de 35 dias) âœ…
* Tire snapshots manuais antes de mudanÃ§as maiores âœ…
* Use read replica cross-region para sistemas crÃ­ticos âœ…
* Teste DR trimestralmente (restore, promova, meÃ§a) âœ…

Design de AplicaÃ§Ã£o:
* LÃ³gica de retry para conexÃµes âœ…
* TransaÃ§Ãµes idempotentes âœ…
* Health checks e monitoramento âœ…

Custos:
* Multi-AZ: ~2x custo Single-AZ (~R$ 1.500/mÃªs para setup tÃ­pico)
* ROI: Se paga prevenindo <1 hora downtime/mÃªs

MÃ©tricas CrÃ­ticas:
* Monitore: DatabaseConnections, ReplicaLag, FreeStorageSpace
* Alerte: Lag >60s, Storage <10 GB, ConexÃµes >80%